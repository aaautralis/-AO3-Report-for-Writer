import json
import re
import time
from datetime import datetime
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# from tqdm import tqdm  <-- æŠŠè¿™è¡Œæ³¨é‡Šæ‰
def tqdm(iterable, **kwargs): return iterable # <-- åŠ ä¸Šè¿™ä¸€è¡Œï¼Œè¿™å°±å«â€œå‡è¿›åº¦æ¡â€

# ================= é…ç½®åŒº =================
DATA_FILE = "my_ao3_db.json"
USER_DATA_DIR = "chrome_user_data"
# =========================================

def parse_int(s):
    """æå–å­—ç¬¦ä¸²ä¸­çš„æ•°å­—"""
    if not s: return 0
    return int(re.sub(r"[^\d]", "", s))

def clean_text(text):
    return text.strip() if text else ""

def get_categories(soup):
    """æå–åˆ†ç±» (Category) - è¿”å›åˆ—è¡¨"""
    tags = soup.select("ul.required-tags span.category")
    cats = []
    for tag in tags:
        val = tag.get("title", "").strip()
        if val: cats.append(val)
    return cats if cats else ["Unknown"]

def get_rating(soup):
    """æå–åˆ†çº§ (Rating)"""
    tag = soup.select_one("ul.required-tags span.rating")
    return tag["title"].strip() if tag and tag.get("title") else "Unknown"

def get_recursive_comments(soup, current_chapter_default=1):
    """é€’å½’è§£æè¯„è®ºæ ‘ (å¸¦ç²¾å‡†çš„ chapter_index)"""
    comments_flat_list = []

    def parse_thread(thread_ol, parent_id=None, current_chapter_idx=current_chapter_default):
        if not thread_ol: return

        all_lis = thread_ol.find_all("li", recursive=False)
        i = 0
        while i < len(all_lis):
            li = all_lis[i]
            raw_id = li.get("id")
            
            if raw_id and raw_id.startswith("comment_"):
                my_id = raw_id.replace("comment_", "")
                user = "Guest"
                chapter_idx = current_chapter_idx 
                chapter_name = f"Chapter {chapter_idx}"
                date_str = ""
                
                byline = li.find("h4", class_="byline")
                if byline:
                    user_link = byline.find("a", href=re.compile(r"^/users/"))
                    if user_link: user = user_link.get_text(strip=True)
                    
                    # ç²¾ç¡®æå–ç« èŠ‚ Index
                    byline_text = byline.get_text()
                    match = re.search(r"on Chapter\s+(\d+)", byline_text)
                    if match:
                        chapter_idx = int(match.group(1))
                        chapter_name = f"Chapter {chapter_idx}"
                    else:
                        chapter_idx = 1
                        chapter_name = "Chapter 1"

                dt_span = li.find("span", class_="datetime")
                if dt_span: date_str = dt_span.get_text(strip=True)

                block = li.find("blockquote", class_="userstuff")
                text_content = clean_text(block.get_text("\n")) if block else "[Deleted/Hidden]"

                comments_flat_list.append({
                    "id": my_id,
                    "parent_id": parent_id,
                    "user": user,
                    "chapter_index": chapter_idx,
                    "chapter_name": chapter_name,
                    "date": date_str,
                    "text": text_content[:500] 
                })
                
                if i + 1 < len(all_lis):
                    next_li = all_lis[i + 1]
                    if not next_li.get("id"): 
                        reply_ol = next_li.find("ol", class_="thread")
                        if reply_ol:
                            parse_thread(reply_ol, parent_id=my_id, current_chapter_idx=chapter_idx)
                            i += 1 
            i += 1

    placeholder = soup.find("div", id="comments_placeholder")
    if placeholder:
        root_thread = placeholder.find("ol", class_="thread", recursive=False)
        if root_thread:
            parse_thread(root_thread)
    
    return comments_flat_list

def main():
    print("ğŸš€ AO3 å¹´åº¦æ€»ç»“æŠ“å–å·¥å…· [v2.3 Unrevealed Fix]")
    print("âœ¨ ä¿®å¤: Unrevealedä½œå“ä¹Ÿèƒ½æ­£ç¡®æŠ“å–å®Œæ•´ç« èŠ‚åˆ—è¡¨")
    
    with sync_playwright() as p:
        context = p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=False, 
            viewport={'width': 1280, 'height': 800}
        )
        page = context.pages[0]

        # ================= 1. ç™»å½•éªŒè¯ =================
        print("ğŸ”— æ­£åœ¨éªŒè¯èº«ä»½...")
        page.goto("https://archiveofourown.org/")
        
        user_greeting = page.locator("#greeting ul.user.navigation a[href^='/users/']").first
        if user_greeting.count() == 0:
            print("\nğŸš¨ è¯·å…ˆæ‰‹åŠ¨ç™»å½• (å‹¾é€‰Remember Me)ï¼Œå®ŒæˆåæŒ‰å›è½¦...")
            input()
            page.reload()
            user_greeting = page.locator("#greeting ul.user.navigation a[href^='/users/']").first
            if user_greeting.count() == 0:
                print("âŒ ç™»å½•å¤±è´¥ï¼Œé€€å‡ºã€‚")
                context.close()
                return

        href_val = user_greeting.get_attribute("href") 
        current_user = href_val.split("/")[-1]
        print(f"âœ… å½“å‰ç”¨æˆ·: ã€{current_user}ã€‘")

        full_data = {
            "account": {
                "username": current_user,
                "fetch_time": datetime.now().isoformat(),
            },
            "works": []
        }

        # ================= 2. Stats (éšå½¢æ•°æ®) =================
        print("\nğŸ“Š [1/3] è·å– Stats (è®¢é˜…/æ”¶è—)...")
        stats_map = {}
        try:
            page.goto(f"https://archiveofourown.org/users/{current_user}/stats")
            soup = BeautifulSoup(page.content(), "html.parser")
            for lnk in soup.select("a[href*='/works/']"):
                row = lnk.find_parent("li")
                if not row: continue
                wid = lnk['href'].split("/")[-1]
                txt = row.get_text()
                if wid not in stats_map:
                    subs = parse_int(re.search(r"Subscriptions:\s*(\d+)", txt).group(1)) if "Subscriptions:" in txt else 0
                    bms = parse_int(re.search(r"Bookmarks:\s*(\d+)", txt).group(1)) if "Bookmarks:" in txt else 0
                    stats_map[wid] = {"subs": subs, "bookmarks": bms}
        except Exception as e:
            print(f"   âš ï¸ Stats è·å–å¤±è´¥: {e}")

        # ================= 3. æ‰«æåˆ—è¡¨ (Metaä¿¡æ¯) =================
        print("\nğŸ“‹ [2/3] æ‰«æä½œå“åˆ—è¡¨...")
        work_list_skeleton = []

        def scan_list(url_suffix, label, max_pages=10):
            base_url = f"https://archiveofourown.org/users/{current_user}/{url_suffix}"
            print(f"   > æ‰«æ {label} ...")
            page_num = 1
            try:
                while page_num <= max_pages:
                    url = base_url if page_num == 1 else f"{base_url}?page={page_num}"
                    print(f"     - Page {page_num}")
                    page.goto(url, timeout=60000)
                    # page.goto(base_url)
                    if page.locator("text='Proceed'").count() > 0: page.click("text='Proceed'")
                    
                    soup = BeautifulSoup(page.content(), "html.parser")
                    items = soup.select("li.own.work.blurb") 
                    if not items:
                        return
                
                    for item in items:
                        # 1. è·å–æ›´æ–°æ—¥æœŸå¹¶è¿›è¡Œå¹´ä»½æ£€æŸ¥
                        dt = item.find("p", class_="datetime")
                        if not dt:
                            continue
                        date_text = dt.text.strip()
                        # æ ¼å¼é€šå¸¸ä¸º "18 Dec 2025"
                        
                        try:
                            # æå–æœ«å°¾çš„ 4 ä½æ•°å­—å¹´ä»½
                            year_match = re.search(r"(\d{4})$", date_text)
                            if year_match:
                                year = int(year_match.group(1))
                                if year < 2025:
                                    print(f"  ğŸ›‘ å‘ç° {year} å¹´ä½œå“ï¼Œåœæ­¢æ‰«æè¯¥åˆ—è¡¨ã€‚")
                                    return # ç›´æ¥è·³å‡ºå½“å‰çš„ scan_list å‡½æ•°
                        except Exception:
                            pass # å¦‚æœè§£æå¤±è´¥ï¼Œç¨³å¦¥èµ·è§ç»§ç»­å¾€ä¸‹èµ°

                        h4 = item.find("h4", class_="heading")
                        if not h4: continue
                        link = h4.find('a')
                        wid = link['href'].split("/")[-1]
                        title = link.text.strip()
                        
                        categories = get_categories(item)
                        rating = get_rating(item)
                        relationships = [r.text for r in item.select("li.relationships a")]
                        # æŠ“å–è‡ªç”±æ ‡ç­¾ (Freeform Tags / Additional Tags)
                        freeform_tags = [t.text for t in item.select("li.freeforms a")]

                        stats_dl = item.find("dl", class_="stats")
                        chapters_text = "1/1"
                        status = "Completed"
                        if stats_dl:
                            chap_dd = stats_dl.find("dd", class_="chapters")
                            if chap_dd:
                                chapters_text = chap_dd.text.strip()
                                if "/" in chapters_text:
                                    curr, total = chapters_text.split('/', 1)
                                    if total == "?" or curr != total:
                                        status = "In Progress"
                                    else:
                                        status = "Completed"
                        
                        status_span = h4.find("span", class_="status")
                        w_type = "Normal"
                        if status_span:
                            st_text = status_span.text.lower()
                            if "anonymous" in st_text: w_type = "Anonymous"
                            if "unrevealed" in st_text: w_type = "Unrevealed"

                        def gv(c): return parse_int(stats_dl.find("dd", class_=c).text) if stats_dl and stats_dl.find("dd", class_=c) else 0
                        
                        if not any(x['work_id'] == wid for x in work_list_skeleton):
                            work_list_skeleton.append({
                                "work_id": wid,
                                "title": title,
                                "url": link['href'],
                                "work_type": w_type,
                                "rating": rating,
                                "categories": categories,
                                "relationships": relationships,  # âœ… è¡¥å…¨å…³ç³»
                                "freeform_tags": freeform_tags,  # âœ… è¡¥å…¨è‡ªç”±æ ‡ç­¾
                                "status": status,
                                "chapters_text": chapters_text,
                                "fandoms": [t.text for t in item.select("h5.fandoms a")],
                                "words": gv("words"),
                                "kudos": gv("kudos"),
                                "hits": gv("hits"),
                                "comments_count": gv("comments"),
                                "date_updated": item.find("p", class_="datetime").text.strip(),
                                "real_subs": stats_map.get(wid, {}).get("subs", 0),
                                "real_bookmarks": stats_map.get(wid, {}).get("bookmarks", 0),
                                "chapters_detail": [] 
                            })
                    page_num += 1
                    time.sleep(2)  # ğŸ‘ˆ éå¸¸é‡è¦
            except Exception as e:
                print(f"   âš ï¸ æ‰«æ {label} å‡ºé”™: {e}")

        scan_list("works", "ä¸»é¡µä½œå“")
        scan_list("works/collected", "åˆé›†ä½œå“")
        print(f"   âœ” å…±å‘ç° {len(work_list_skeleton)} ç¯‡ä½œå“")

        # ================= 4. æ·±åº¦æŠ“å– =================
        print("\nğŸ•µï¸ [3/3] æ·±åº¦æŠ“å– (ç« èŠ‚è¯¦æƒ… & è¯„è®ºæ ‘)...")
        
        final_works = []
        for w in tqdm(work_list_skeleton, desc="Processing"):
            try:
                # --- Step A: æŠ“å–ç« èŠ‚è¯¦æƒ… (/navigate) ---
                # ã€ä¿®æ”¹ç‚¹ã€‘: ç§»é™¤äº†å¯¹ "Unrevealed" çš„è¿‡æ»¤ï¼Œè®©æ‰€æœ‰ä½œå“éƒ½å°è¯•æŠ“å– navigate
                # å› ä¸ºä½œè€…æœ¬äººæœ‰æƒé™çœ‹åˆ° Unrevealed ä½œå“çš„ç« èŠ‚åˆ—è¡¨
                
                nav_url = f"https://archiveofourown.org{w['url']}/navigate"
                page.goto(
                nav_url,
                timeout=60000,
                wait_until="domcontentloaded"
            )

                if page.locator("text='Proceed'").count() > 0: page.click("text='Proceed'")
                
                # æ£€æŸ¥ URL æ˜¯å¦è¿˜åœ¨ navigate é¡µé¢ (å•ç« ä½œå“ä¼šè‡ªåŠ¨é‡å®šå‘å›ä¸»é¡µ)
                if "/navigate" in page.url:
                    soup_nav = BeautifulSoup(page.content(), "html.parser")
                    chap_items = soup_nav.select("ol.chapter.index li")
                    
                    for idx, li in enumerate(chap_items, 1):
                        date_span = li.find("span", class_="datetime")
                        c_date = date_span.text.strip("()") if date_span else ""
                        c_link = li.find("a")
                        c_title = c_link.text.strip() if c_link else f"Chapter {idx}"
                        
                        w["chapters_detail"].append({
                            "chapter_index": idx,
                            "chapter_title": c_title,
                            "publish_date": c_date
                        })
                else:
                    # å¦‚æœé‡å®šå‘äº†ï¼Œè¯´æ˜æ˜¯å•ç«  (æˆ–è€…æå°‘è§çš„ç‰¹æ®Šéšè—æƒ…å†µ)ï¼Œç”¨æ›´æ–°æ—¥æœŸå…œåº•
                    w["chapters_detail"].append({
                        "chapter_index": 1,
                        "chapter_title": w['title'], # å•ç« ä½œå“æ²¡æœ‰ç« èŠ‚åï¼Œç”¨ä½œå“å
                        "publish_date": w['date_updated']
                    })
                
                # å¦‚æœ Unrevealed æŠ“å–æˆåŠŸï¼Œchapters_detail åº”è¯¥æœ‰ 25 é¡¹äº†
                
                # --- Step B: æŠ“å–å…¨æ–‡ä¸è¯„è®º ---
                full_url = f"https://archiveofourown.org{w['url']}?view_full_work=true&show_comments=true&view_adult=true"
                page.goto(full_url, timeout=60000)
                if page.locator("text='Proceed'").count() > 0: 
                    page.click("text='Proceed'")
                    page.wait_for_load_state("domcontentloaded")

                soup = BeautifulSoup(page.content(), "html.parser")

                # è¡¥å…¨é¦–æ¬¡å‘å¸ƒæ—¶é—´
                if w["chapters_detail"]:
                    w["first_published"] = w["chapters_detail"][0]["publish_date"]
                else:
                    meta_published = soup.select_one("dl.work.meta.group dd.published")
                    w["first_published"] = meta_published.get_text().strip() if meta_published else w.get("date_updated", "")

                # æŠ“å– Kudos
                try:
                    if page.locator("#kudos_summary a:has-text('others')").count() > 0:
                        page.click("#kudos_summary a:has-text('others')")
                        page.wait_for_timeout(500)
                        soup = BeautifulSoup(page.content(), "html.parser")
                except: pass

                kudos_els = soup.select("#kudos a[href^='/users/']")
                w["kudos_givers"] = [k['href'].split("/")[-1] for k in kudos_els]

                # æŠ“å–è¯„è®ºæ ‘
                comments_tree = get_recursive_comments(soup)
                w["comments_tree"] = comments_tree
                
                w["commenters"] = [
                    {"user": c["user"], "chapter_index": c["chapter_index"]}
                    for c in comments_tree
                ]

                final_works.append(w)
                time.sleep(1) 

            except Exception as e:
                print(f"âŒ é”™è¯¯ã€Š{w['title']}ã€‹: {e}")
                final_works.append(w) 

        # 5. ä¿å­˜
        full_data["works"] = final_works
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(full_data, f, ensure_ascii=False, indent=2)

        print("\n" + "="*50)
        print(f"ğŸ‰ æŠ“å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ {DATA_FILE}")
        print("="*50)
        context.close()

if __name__ == "__main__":
    main()